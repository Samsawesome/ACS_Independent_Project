Literature review: The Modern Storage Mixup and how IO Stacks must Evolve

The historical storage hierarchy that has been almost constant for more than 30 years has become disrupted by the continued pursuit of better, faster technology. The overlap in latency of persistent memory (PM) with future-gen ultra low latency SSDs (in modern day, Optane SSDs) has created what Huang et al. call the "Storage Jungle" [3]. Previously distinct boundaries between memory types have become blurred as slower tech has continued to increase in speed thanks to technological improvements. This improvement presents massive implications for kernel IO stacks, memory data structures, and overall system design. This literature review summarizes multiple papers that together show this new modern landscape, and together show how OS/kernel software overhead has become the bottleneck in IO.

Huang et al. identified two disruptive trends reshaping the storage landscape [3]. First, PM technologies like 3D XPoint bring byte-addressable persistence to the memory bus with latencies as low as 300ns. Second, next-generation SSDs increasingly approach PM performance, NVMe-based Optane SSDs achieve less than 6us latency. This convergence creates what the authors call a "storage jungle" rather than a hierarchy, with significant implications for persistent data structure design, due to there no longer being a clear order of which memory types have a lower or higher latency than the others. However, just because these memory types have similar latencies does not mean that they have similar cost effectivnesses. Huang et al. demonstrated that PM based systems proved less cost effective than a traditional DRAM SSD hierarchy [3]. While Optane DCPMM raw media costs only 1.6 times more per GB than Optane SSDs, the total system cost is much higher. PM's unconventional installation requirements, such as including mandatory DRAM co-population, lead to significant overprovisioning. When accounting for the overall system, a single DCPMM system costs ~14-18 dollars per GB, depending on thread count. This is compared to ~4 dollars per GB for a dual Optane SSD system. Overall, while faster methods exist, just looking at latency minimization does not give the full picture of other costs to the system (or the owner of the system).

Lee et al. showed how with ultra low latency SSDs, the kernel IO stack accounts for ~35% of total IO latency [2]. The breakdown of 4KB random access reads on Optane SSDs showed that ~37% of time is spent in kernel processing. This is almost an inverse of historical IO latency profiles (where kernel processing time was negligible). To further show the overhead of kernel processing, Ren and Trivedi perform a perf characterization of Linux's IO APIs on Optane SSDs [1]. Their analysis showed significant differences in efficiency across API models, but all introduced substantial overhead on fast storage devices. APIs that were designed for rotational media with seek time optimization in mind struggle with the low latencies of modern SSDs. And on NUMA APIs, scalability issues emerge, mostly from the global lock contention. Overall, APIs that have been designed in the past that assumed that kernel processing time is negligible are having issues as modern SSDs get faster and faster, and new APIs or techniques must be designed and used to fully exploit the increased speed of modern SSDs.

Lee et al. proposed AIOS(Asynchronous IO Stack), a fundamental reorder of the kernel IO stack that overlaps CPU operations with device IO [2]. It includes a lightweight block IO layer, which is simplified for NVMe SSDs. This layer removes request merging and reordering, along with IO scheduling, reducing submission latency by ~85%. It also includes asynchronous page allocation, DMA mapping, and overlapped journaling operations. All of this led to AIOS having 15-33% lower latency for random access reads and 21-26% lower latency for random access writes. Another studied technique, persistent indexes, was studied by Huang et al., and it compared PM tailed designs with SSD based structures with DRAM buffer pools [3]. Huang et al. found that for memory resident workloads, SSD based systems are more cost effective, but for partial memory workloads, PM designs can be more cost effective, especially when interleaved between multiple DCPMMs. Overall, there are techniques that have been made and exist to improve the kernel IO stack, but they usually come at the price of increased cost, or push their complexity onto the SSD.

These three papers explored show the fundamental tension in modern storage systems, the fight between improved latency and minimized costs. The well known historical storage hierarchy has evolved towards something more resemblant of a storage jungle, where performance boundaries overlap and optimal methods rely more on cost effectiveness than minimum latency. Huang et al. establishes the cost performance landscape, revealing PM's economic challenges despite latency improvements [3]. Ren and Trivedi characterized I/O API trade-offs, and Lee et al. demonstrated that substantial kernel optimization is possible through asynchronous design [1,2]. Collectively, these works show that there is not yet a dominant technology to improve the IO stack. Depending on the type of data, type of commands, type of system, and type of workload results in different techniques being better than others. In conclusion, as storage latencies continue to decrease, new techniques and technologies must be invented to continue to allow the improved storage technologies to shine.


