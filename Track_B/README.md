# READ ME
## Generated by: Sam Slane  
## For: Advance Computer Systems  
## Project: Microarchitectural Modeling and OS Interface of an IOPS-Optimized Domain-Specific Core  


### Project Statement
This project proposes to investigate, model, and quantify the potential of a IOPS core microarchitecture on the host's side. The core research question is: how can the co-design of a lightweight host microengine and a minimalistic interface maximize the effective IOPS and reduce tail latency for AI IO patterns, when interfacing with ultra low latency next-gen SSDs? The focus will be on microarchitectural modeling (creating an optimized simulated engine to perform IO processing), data collection (recording the time spent in block layer during actual Windows reads/writes), and performance quantification (comparing the two and displaying differences via tables and graphs). This project's deliverables are a simulation model(```involved_hardware_blockqueue.sv```), an analytical report(```report.md```), a literature review of relavent papers(```Notes/literature_review.md```), and a visualization of the compared data between hardware and software(```Figures/*```).

### Solution Description
The solution created is a hardware simulation created in SystemVerilog. My idea was to take the current block layer (including the queue and NVMe drivers) in the Windows IO stack and to copy it into hardware. This way the process could be parallelized, and also just be faster in general due to the lack of software/kernel/OS overhead. Implemented in SystemVerilog is a IRP manager, block layer processor, and an NVMe driver, along with performance gathering modules and a testbench. The IRP manager, block layer processor, and NVMe driver were all modeled in hardware to be as accurate as possible to their software equivalents in the kernel. Major changes between what the kernel does and what my hardware does is the parallelization of the process. The entire process was pipelined so that as many commands could process as possible at the same time, leaving latency at ~7.5us, but cycles/time per command at ~22.5 cyles or ~225 ns per command. Meaning that while one command took ~7.5 us to process, they were so parallelized that a command was being output every ~225 ns. One thing that my solution lacks is a submission and completion queue, but I did not have enough time to add that to my hardware simulation, and plan on adding it as the next step to this project next semester.  

For summary: The innovation of this project is in transfering part of the IO stack from software to hardware, parallelizing the process and removing software overhead.

### Navigating Folders  
All code can be found in the base Track_B folder.  
Graphs are found in the Figures folder.  
Commands run by the code are found in the Commmands folder.  
Outputs generated by the code are found in the Outputs folder.  
Leftover notes and the literature review can be found in the Notes folder.  
Overall report can be found in the main directory and is labeled ```report.md```.

### Running the Code  

**To generate a new software_output.txt:**  
Change the SSD latency to your SSD's latency in analysis.py (lines 24 and 25)  
Change the CPU frequency to your CPU's frequency in analysis.py (line 66)  

Optional: Run run_io_trace.bat and use Windows Performance Recorder to generate a new .etl file for your computer.  
Optional: Then find the average disk service time in Windows Performance Analyzer and calculate block layer overhead from that number.  
Optional: Replace BLOCK_LAYER_OVERHEAD_PER_CMD in analysis.py (line 20) with the new calculated value (in ns)  

Run software_build.bat in CMD (```software_build.bat```)
Run block_layer_test.exe in CMD (```block_layer_test.exe```)

**To generate a new hardware_output.txt:**  
In line 1898/1899 in involved_hardware_blockqueue.sv, change the path to where you have this project stored. You can also choose which input file to run.  
I recommend running 70_cpu_commands.txt as it shows more improved IOPS and throughput from the parallelism.  

Simulate involved_hardware_blockqueue.sv in ModelSim. Do not run it using iverilog (Icarus Verilog), as it does not have the ability to run this file.

**To generate new figures:**  
Once software_output.txt and hardware_output.txt have been generated, run analysis.py in cmd (```python analysis.py```).